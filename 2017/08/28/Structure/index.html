<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Is Relu Useful in Network Quantization? | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Relu is a widely used non-linear activation function in Recently Neural Network. Relu is not a neccesary layer after doing convolution operation, but it forces those negetive values of feature map to">
<meta property="og:type" content="article">
<meta property="og:title" content="Is Relu Useful in Network Quantization?">
<meta property="og:url" content="http://yoursite.com/2017/08/28/Structure/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Relu is a widely used non-linear activation function in Recently Neural Network. Relu is not a neccesary layer after doing convolution operation, but it forces those negetive values of feature map to">
<meta property="og:updated_time" content="2017-08-28T07:19:20.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Is Relu Useful in Network Quantization?">
<meta name="twitter:description" content="Relu is a widely used non-linear activation function in Recently Neural Network. Relu is not a neccesary layer after doing convolution operation, but it forces those negetive values of feature map to">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Structure" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/28/Structure/" class="article-date">
  <time datetime="2017-08-28T05:58:41.000Z" itemprop="datePublished">2017-08-28</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Is Relu Useful in Network Quantization?
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Relu is a widely used non-linear activation function in Recently Neural Network. Relu is not a neccesary layer after doing convolution operation, but it forces those negetive values of feature map to be zero, results in the sparity of feature map<br>and keeps all activation to become non-negative.<br>Intuitively, in quantization task, relu reduces the quantization range, which allows low bit to express numbers more accurately. But can relu always benifit the quantization of CNN?</p>
<p>Maybe it is useful in many network structures, and in fact I have benefitted from it in quantizing vgg-like Network. But is not for some special cases. Let’s assume a structure, a 3x3 convolution followed by a 1x1 convolution, which<br>is commonly used in ResNet and so on. There is a special property of 1x1 convolution that it doesn’t expand the receptive field of feature map, which we can easily merge the 1x1 convolution into 3x3 convolution, if there is no relu layer inserted between them.</p>
<p>Compare this two kind of structure, one is 3x3, $$i$$ input channels and $$t$$ output channels, followed by 1x1, $$t$$ input channels and $$o$$ output channels. Another is a merged version of convolution, with 3x3 kernel size, $$i$$ input channels and $$o$$ output channels. We compare this two kinds of structure by estimating their computation amount, parameter amout.</p>
<ul>
<li>computation amount: 3x3xtxi+1x1xtxo vs. 3x3xixo</li>
<li>parameter amount: 3x3xtxi+1x1xtxo vs. 3x3xixo</li>
</ul>
<p>As we can see, if $$t$$ is larger than $$o$$, there is no doubt that merged version can save computation resources. </p>
<p><strong>GPU Performance</strong><br>We implement the convolution using caffe framework. In caffe, convolution is transformed into a matrix multiplication. It computes the result parallelly on output channel dim.<br>Experiment results show that reducing output channel cannot significantly lower the running time. All we can benefit from the merged version is the removal of 1x1 convolution and time can be saved.</p>
<p><strong>FPGA Performance</strong><br>FPGA cannot completely compute parallelly on out channel dimension. It’s obiviously that reducing the output channel can reduce loop trip count, and it can also reduce cost of data load and save, which is often the bottleneck of FPGA design. But if $$t &lt; o$$, there will be an increase in computation amount, which may unfortunately cost more time.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/28/Structure/" data-id="cj6vu2smn0002lf4ig4dnlcid" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/08/25/Network/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">Low-Bit Quantization Strategy of Convolution Neural Network</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/08/28/Structure/">Is Relu Useful in Network Quantization?</a>
          </li>
        
          <li>
            <a href="/2017/08/25/Network/">Low-Bit Quantization Strategy of Convolution Neural Network</a>
          </li>
        
          <li>
            <a href="/2017/08/25/test/">test</a>
          </li>
        
          <li>
            <a href="/2017/08/25/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>