<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Low-Bit Quantization Strategy of Convolution Neural Network | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="MotivationIt has been well shown that quantizing a convolution neural network can significant reduce the overhead of computation on hardware. A hardware friendly quantization strategy involves fixed-p">
<meta property="og:type" content="article">
<meta property="og:title" content="Low-Bit Quantization Strategy of Convolution Neural Network">
<meta property="og:url" content="http://yoursite.com/2017/08/25/Network/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="MotivationIt has been well shown that quantizing a convolution neural network can significant reduce the overhead of computation on hardware. A hardware friendly quantization strategy involves fixed-p">
<meta property="og:updated_time" content="2017-08-28T05:57:21.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Low-Bit Quantization Strategy of Convolution Neural Network">
<meta name="twitter:description" content="MotivationIt has been well shown that quantizing a convolution neural network can significant reduce the overhead of computation on hardware. A hardware friendly quantization strategy involves fixed-p">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  

</head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Network" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/25/Network/" class="article-date">
  <time datetime="2017-08-25T10:56:04.000Z" itemprop="datePublished">2017-08-25</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Low-Bit Quantization Strategy of Convolution Neural Network
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>Motivation</strong><br>It has been well shown that quantizing a convolution neural network can significant reduce the overhead of computation on hardware. A hardware friendly quantization strategy involves fixed-point, few multiplication and low bitwise. Many researches have shown that quantizing a full precision CNN into 8bit can achieve little degradtion in accuracy. And another extreme low bit scheme like Binary-Wise Network, Binary Neural Network, XNOR Network also shows that low-bit quantization preform well in some extent. </p>
<p>Since fixed-point quantization and bitwise operation give a opportunity to break the limitation of computation resources such as dsp on FPGA,  the bandwidth becomes the deterministic factor of the computation efficiency. Low bit wise quantization apparently can mitigate the load of bandwidth</p>
<p><strong>activation quantization</strong><br>Efficient bit wise is usuaslly integer power of 2, in order to make good use of the whole bit width of the bus. We consider a 4bit quantization scheme to transfer feature map into computation core. Usual quantization of 4bit is dynamic fixed point quantization. And in tensorflow, they quantize numbers into a linear distribution among the range of activations. In our implement, we try to use the code book and loop-up table on FPGA to quickly quantize activations.</p>
<p>The code book contains 16 numbers when 4bit quantization scheme is utilized. The way to find code book is simply use K-Means to optimize the F-norm function:<br>$$<br>\min||\mathbf{a}-q\mathbf{(a)}||_{F}<br>$$</p>
<p>Although there is a limitation of the range which 4bit codebook can depict, the result shows that the outlier of feature map won’t significantly impact the computation output,<br>except the top fc-layer. Some reseach have shown that in classification task, low bit can still work well. But on segmentation task, each pixel will be taken into consideration<br>independly, unlike classification task, which only needs a single output.  In our experiment, we show that quantizing the fc layer into 4bit codebook before it is fed into softmax layer will cause a 3% reduction on the accuracy.<br>compare to 6bit dynamic fixed-point quantization. So it’s merging softmax layer into fc layer on FPGA that be a more better way to quantize the network into 4bit.</p>
<p><strong>weight quantization</strong><br>Quantizing weight into low bit is not difficult. Many related work like Binary Neural Network, Tenary Neural Network have show impressive performances even when weights are quantized into extreme low bit, and some work show quantize weight into 2-power can also work well, while in my project I cannot reproduce its performance by finefuning the full precision network.<br>To obtain the benefits of 2-power weight, I quantize the weight into the sum of two 2-power number. This strategy will cost 8 bits in my design, but there is no gap to finetune it from a full precision network. The burden of 8 bit weight seems not to do harm to the transfer efficiency because the amount of weight is relatively small.<br>In my experiment, this kind of quantize strategy is nearly lossless in mIOU measurement of segementation task.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/08/25/Network/" data-id="cj6vu2sme0000lf4ie4sqhc7w" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/08/28/Structure/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Récent</strong>
      <div class="article-nav-title">
        
          Is Relu Useful in Network Quantization?
        
      </div>
    </a>
  
  
    <a href="/2017/08/25/test/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Ancien</strong>
      <div class="article-nav-title">test</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/08/28/Structure/">Is Relu Useful in Network Quantization?</a>
          </li>
        
          <li>
            <a href="/2017/08/25/Network/">Low-Bit Quantization Strategy of Convolution Neural Network</a>
          </li>
        
          <li>
            <a href="/2017/08/25/test/">test</a>
          </li>
        
          <li>
            <a href="/2017/08/25/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>